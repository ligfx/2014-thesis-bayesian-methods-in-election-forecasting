\documentclass[thesis.tex]{subfiles}
\begin{document}

\section{Gibbs sampling}

Consider a bivariate random variable \((x, y)\), for which we want to find the joint distribution \begin{equation}\begin{aligned}
	p(x, y).
\end{aligned}\end{equation} It's easier to calculate the conditional distributions \(p(x | y)\) and \(p(y | x)\) than to integrate to get the marginal, \(p(x) = \int p(x, y) dy\).

The Gibbs sampling algorithm for two variables is as follows:

\begin{enumerate}
	\item Start with some value for \(y\), \(y^{(0)}\);
	\item Sample \(x^{(0)} \sim p(x | y = y^{(0)})\);
	\item Sample \(y^{(1)} \sim p(y | x = x^{(0)}\);
	\item And so on.
\end{enumerate}

The general multivariate version is analagous.

In the next section we provide a concrete example of finding the conditional distributions for the Gibbs sampler. In general, we find the full joint distribution, which by definition is proportional to the conditional distribution of each variable. Then for each conditional distribution, we simply drop unneeded terms into the normalization constant and hope to get a conjugate posterior.

\bigskip

\noindent\textbf{Claim:} The Gibbs sampler is a Markov process whose stationary distribution is the actual joint distribution. We show the detailed balance equations hold for sampling \(x\) with regards to the joint distribution \(p(x, y)\), \begin{equation}\begin{aligned}
	p(x_2, y) p(x_1| y) = p(x_1, y) p(x_2 | y).
\end{aligned}\end{equation} By the definition of conditional probability, this is \begin{equation}\begin{aligned}
	p(x_2 | y) p(y) \cdot p(x_1| y) = p(x_1 | y) p( y) \cdot p(x_2 | y),
\end{aligned}\end{equation} so we're done.

\end{document}