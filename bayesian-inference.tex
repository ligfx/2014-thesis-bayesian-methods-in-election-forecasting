\documentclass[thesis.tex]{subfiles}
\begin{document}

\section{Bayesian inference}

\subsection{Incorporating opinion polls}

In the American presidential elections, opinion polls are provided on both the national and state level by an assortment of different organizations. The models in this paper use polling data to estimate an accurate picture of true voter opinion.

Polls come with a few problems that models have to deal with. Polls themselves are not as accurate as them claim to be. People change their minds over the election cycle. Different organizations have different biases and inaccuracies, stemming from either political leanings or different methods of sampling. All organizations have to estimate who will end up voting to weight their polls appropriately.

We first examine the model described by \cite{Lock:2010aa}, which uses simple Bayesian inference to incorporate polls based on estimated accuracy some months before Election Day. It doesn't handle organizational biases, but it makes up for it with simplicity and the intuitiveness of the math.

After that, we examine the the model by \cite{Strauss:2007aa}, which uses Gibbs sampling to fit a more involved forecasting model, which estimates organizational biases on the fly.

\begin{comment}
- problem: how accurate/useful are they? how many months before campaign (problems 
  with changing mind, making up mind, uncertainty about who will vote)

  (Lock)'s approach: Bayesian inference

  (Strauss): Gibbs

- problem: organization biases. empirically, political leanings, or different 
  methods of polling/sampling, guessing who will actually vote

  (Strauss) Gibbs
  Nate Silver: historical data
\end{comment}

\subsection{Updating beliefs using additional data}

To incorporate polling data with predictions from fundamentals, we use the method of Bayesian inference.

Given a prior distribution over some outcome, \(\Pr(\theta)\), and data \(\vec{x}\), we use Bayes' Rule \[ \Pr(A | B) = \frac{\Pr(B | A) \Pr (A)}{\Pr(B)} \] to get our posterior distribution. We say that \[
	p(\theta | \vec{x}) \propto L(\vec{x} | \theta) p(\theta),
\] where we drop the denominator because it doesn't include \(\theta\) in any way and thus becomes part of the uniquely-determined normalization constant for the distribution.

\begin{comment}
\textbf{Claim:} (note we can drop the bottom part, because it doesn't include \(\theta\). Probability functions are uniquely determined by their inside minus any multiplicative constant. Why is this? Properties of integrals.)
\end{comment}

\begin{comment}
We can give an example here, of coin flipping, which I don't know if it's interesting or not (and not really relevant).
\end{comment}

Luckily, we don't have to necessarily write out probability density functions and ``read off'' what family the posterior distribution is. Conjugate families of distributions provide an easy method for calculating posterior distributions based on the parameters of the prior and the data. We show two useful conjugate families, the Normal distribution with unknown mean, and the Normal distribution with unknown precision (the inverse of variance).

\bigskip
\bigskip

\noindent\textbf{Normal with unknown mean:} Given a distribution for data $x \sim \N(\mu, \tau^{-1})$ and a prior on the mean $\mu \sim \N(\mu_0, \tau_0^{-1})$, we see that: \begin{align*}
\Pr(\mu | x, \tau) &\propto \Pr(x | \mu, \tau)\Pr(\mu) \\
&\propto \exp\left( -\frac{\tau(x-\mu)^2}{2} \right)\exp\left( -\frac{\tau_0(\mu - \mu_0)^2}{2} \right) \\
&= \exp\left[ -\frac12\left( \tau x^2 - 2x\mu\tau + \tau\mu^2 + \tau_0\mu^2 - 2\mu\mu_0\tau_0 + \tau_0\mu_0^2 \right) \right]
\intertext{All terms not involving $\mu$ can be dropped, because they'll work out in the normalization constant.}
&\propto \exp\left[  -\frac12(\mu(\tau + \tau_0) -2\mu(x\tau + \mu_0\tau_0)  \right] \\
&= \exp\left[  -\frac12(\tau_0 + \tau)\left(  \mu - \frac{x\tau + \mu_0\tau_0}{\tau_0 + \tau}  \right)^2\right]
\end{align*}
So the posterior distribution is: \[
\mu | x, \tau \sim \N\left(  \frac{x\tau + \mu_0\tau_0}{\tau_0 + \tau}, \text{ }\tau_0 + \tau  \right).
\]

\bigskip
\bigskip

\noindent\textbf{Normal with unknown precision:} Given a a distribution for $n$ i.i.d. data points $x_i \sim \N(\mu, \tau)$, and a prior on the precision $\tau \sim \Gamma(\alpha, \beta)$, we see that: \begin{align*}
\Pr(\tau | \vec{x}, \mu) &\propto \Pr(\vec{x} | \tau, \mu)\Pr(\tau) \\
&\propto \prod\left(\tau^{\frac{1}{2}}\exp\left(  -\frac{\tau(x_i - \mu)^2}{2}  \right)\right) \tau^{\alpha-1}\exp(-\beta\tau) \\
&= \tau^{\alpha + \frac{n}{2} - 1}\exp\left(  -\tau\left(\beta + \frac{\sum(x_i + \mu)^2}{2}\right)  \right)
\end{align*}
So the posterior distribution is \[
\tau | x, \mu \sim \Gamma\left(  \alpha + \frac{n}{2}, \text{ } \beta + \frac{\sum(x_i + \mu)^2}{2}  \right).
\]

\bigskip
\bigskip

\subsection{Modeling variance in vote intention before an election}


In this section we examine the model proposed by \cite{Lock:2010aa}. The model empirically estimates variance in voter intention at different times before Election Day, and uses these estimates to incorporate polling data into forecasts derived from fundamentals \citep{Hibbs:2008aa}.

Let \(\alpha_t\) denote the true national proportion of people who intend to vote for the Democratic candidate \(t\) months before the election, and let \(\widehat{\alpha}_t\) denote an estimate of this value from a poll.

The poll variance is the variance of a mean of \(q\) Bernoulli samples, or of the ratio of a Binomial sample and \(q\). We see this because, for a Binomial(\(q\), \(\mu\)) sample, \(qy\), \begin{equation}\begin{aligned}
  \Var(y) = \frac{1}{q^2}qy(1-y) = \frac{y(1-y)}{q}.
\end{aligned}\end{equation}

\bigskip
\bigskip

\noindent\textbf{Definition (Law of total expectation):} \(\E (X) = \E(\E(X | Y))\), by\begin{equation}
\begin{aligned}
	\E(\E(X | Y)) &= \int \left[\int x p(x | y)\, dx\right] p(y)\, dy \\
	&= \iint x p(x, y)\, dx dy \\
	&= \int x \int p(x, y)\, dy dx \\
	&= \int x p(x) \, dx \\
	&= \E(X).
\end{aligned}
\end{equation}

\bigskip

\noindent\textbf{Definition (Law of total variance, or decomposition of variance):} \(\Var(Y) = \E(\Var(Y | X)) + \Var(\E(Y | X))\), by \begin{equation}
\begin{aligned}
	\Var(Y) &= \E(Y^2) - \E(Y)^2 \\
	&= \E(\E(Y^2 | X)) - \E(\E(Y | X))^2 \\
	&= \E\left(\Var(Y|X) + \E(Y | X)^2\right) - \E(\E(Y | X))^2 \\
	&= \E(\Var(Y | X)) + \Var(\E(Y | X)).
\end{aligned}
\end{equation}

\bigskip
\bigskip

\noindent So, by the laws of total variance and total expectation, we see that \begin{equation}\begin{aligned}
	\Var(\widehat{\alpha_t} | \alpha_0)
	&= \E(\Var(\widehat{\alpha_t} | \alpha_t) | \alpha_0) + \Var(\E(\widehat{\alpha_t} | \alpha_t) | \alpha_0) \\
	&= \E\left(\frac{\alpha_t(1- \alpha_t)}{n} | \alpha_0\right) + \Var(\alpha_t | \alpha_0) \\
	&= \frac{\E(\alpha_t | \alpha_0) - \E(\alpha_t^2 | \alpha_0)}{n} + Var(\alpha_t | \alpha_0) \\
	&= \frac{\E(\alpha_t | \alpha_0) + \E(\alpha_t | \alpha_0)^2}{n} + \frac{n-1}{n}\Var(\alpha_t | \alpha_0) \\
	&\approx \frac{\alpha_0 (1 + \alpha_0)}{n} + \Var(\alpha_t | \alpha_0).
\end{aligned}
\end{equation}

We can estimate \(\Var(\alpha_t | \alpha_0)\) empirically using polling data and outcomes from past elections, by calculating the empirical variance of polls and subtracting \(\alpha_0 (1 + \alpha_0) / n\) . Let \(\widehat{\alpha}_{t, i}\) and \(n_{t, i}\) denote estimated vote and sample size for the \(i\)-th poll in month \(t\), and \(N_t\) the number of polls in month \(t\). Then \begin{equation}
\begin{aligned}
\widehat{\Var}(\alpha_t | \alpha_0) = \frac{1}{N_t} \sum_{i=1}^{N_t}\left[ \left( \widehat{\alpha}_{t, i} - \alpha_0\right)^2 - \frac{\alpha_0 (1 - \alpha_0)}{n_{t,i}} \right].
\end{aligned}
\end{equation}

We also want to know \(\Var(\widehat{d}_{s, t} | d_{s, 0}),\) the variance in the relative position \(t\) months before the election of state \(s\). By a similar process, we estimate this using data from the last few election cycles, \begin{equation}\begin{aligned}
	\widehat{\Var}(d_{s, t} | d_{s, 0}) = \frac{1}{elections \cdot 50} \sum_{y}^{elections} \sum_{s=1}^{50} \left[ \left( \widehat{d}_{s, y, t} - d_{s, y, 0}\right)^2 - \frac{\alpha_{s, y, 0} (1 - \alpha_{s, y, 0})}{n_{s, y, t}} \right].
\end{aligned}\end{equation} Polls do not give us \(\widehat{d}_{s, y, t}\), so we  estimate it as \(\widehat{\alpha}_{s, y, t} - \widehat{\alpha}_{y, t}\). 

Now we have our estimates on variance, dependent only on time \(t\).

From before, we have our poll data \begin{equation}\begin{aligned}
\widehat{d_{s, t}} | d_{s, 0} \sim \Normal\left(d_{s, 0}, \frac{\alpha_{s, 0} (1 - \alpha_{s, 0})}{n_{s, t}} + \Var(d_{s, t} | d_{s, 0}\right),
\end{aligned}\end{equation} where we justify normality by the size of the polls. We also have a prior on state deviance from the national outcome, from our multilevel model, \begin{equation}\begin{aligned}
d_{s, 0} | d_{s, previous} \sim \Normal(d_{s, previous} | \Var(d_{s, 0} | d_{s, previous})).
\end{aligned}\end{equation} Because these are both Normal distributions, we end up with a conjugate posterior for state deviation \(d_{s, 0} | data\).

We have a distribution for poll data, \begin{equation}\begin{aligned}
\widehat{\alpha}_t | \alpha_0 \sim \Normal\left( \alpha_0, \frac{\alpha_0 (1 - \alpha_0)}{n_t} + \Var(\alpha_t | \alpha_0) \right),
\end{aligned}\end{equation} and some prior estimated from fundamentals \begin{equation}\begin{aligned}
	\alpha_0 \sim \Normal(\mu_0, \sigma_0^2),
\end{aligned}\end{equation} so again we end up with a simple conjugate posterior.

\begin{comment}
	TODO Should be weighted average on the Var(d_st, d_s0) stuff.
\end{comment}

\cite{Lock:2010aa} complete the model by estimating the distribution of Electoral College outcomes. They simulate 100,000 elections by \begin{enumerate}
	\item Randomly drawing a national popular vote from the national posterior distribution;
	\item Randomly drawing a deviation for each state, where \(\widehat{d}_{s, t} = \widehat{\alpha}_{s, 0} - \alpha_0\);
	\item And calculating the winner of the Electoral College.
\end{enumerate}

\begin{comment}

So, basically, first we look at what \cite{Lock:2010aa} are doing, because it's a nice example for Bayesian inference, and because the approach taken by \cite{Strauss:2007aa} uses Gibbs sampling and gets complicated.

Suppose a poll is centered on the result of an election \(\alpha,\) with some amount of variance \(sigma^2\). Given a prior on the result of the election, which is centered at \(\mu\) with some variance \(v^2\), Bayes' rule tells us that \[
	\alpha | y \sim \N \left[ \left(\frac{\mu}{v^2} + \frac{y}{\sigma^2}\right) \left( \frac{1}{v^2} + \frac{1}{\sigma^2} \right), \left( \frac{1}{v^2} + \frac{1}{\sigma^2} \right) \right].
\]

How do we find out that variance term? Or, how much information does a poll have? (the variance from the election result)

Remember our problems: low reliability earlier in a campaign year, organizational biases.

First we'll talk about low reliability earlier in a campaign year

\end{comment}

\end{document}