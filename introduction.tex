\documentclass[thesis.tex]{subfiles} 
\begin{document}

\section{Introduction}

Nate Silver and other statistically-minded political forecasters garnered much acclaim in the 2012 presidential election cycle for their unashamedly mathematical approach to political punditry and for the resulting accuracy of their models. Silver went on to become something of a pop science celebrity, popularizing his idea of looking at the world through a Bayesian lens \citep{Silver:2012aa}.

In this work we examine different political election forecasting models and the different statistical and mathematical tools behind them.

All models looked at in this work follow a similar strategy of incorporating opinion poll data at various dates before the election, and attempting to track the future possibilities and minimize the variance of their forecasting. In American presidential races, polling data is provided on both a national and state level, and by many different organizations (Gallup, Rasmussen, PPP, etc). Polling data is inherently both inaccurate (a single sample probably will not represent the entire country, and people's opinions can change over a few months) and biased (organizations have political bents).

Some models deal with the biased nature of polling data by simply averaging all available polls under the assumption that biases cancel out \citep{Wang:2012aa} and others specifically analyze each organization and assign a measure of bias \citep{Silver:2012aa}. \citet{Strauss:2007aa} attempts to estimate biases directly from the data during a live campaign, requiring the use of a more computationally intensive model and Markov-Chain Monte Carlo methods.

Polls also suffer from the ``early bird'' problem: people might report their preferences early in the campaign, and then later change their minds. Models typically handle this by using an estimate of how much information polls carry at different dates in a campaign, and initialize their model using a prior forecast from first principles.

\citet{Lock:2010aa}, for example, use historical data on the relevance of polls at different dates in the campaign season (obtained with the help of mixed-effects linear regression, a technique which compensates for small factor samples inside a larger overall sample, described further by \citealt{Gelman:2006aa}) and combines it with a prior forecast based on economic and military events \citep{Hibbs:2008aa}.

\citet{Strauss:2007aa} and \citet{Jackman:2005aa}, on the other hand, develop more complicated models estimating poll variance from theoretical principles as a random walk.

Bayesian statistics, a variant of classical frequentist statistics, provides the theoretical underpinning for many of these models and tools. They allow a model to specify a prior belief, such as macro-level factors in a presidential election (like economic and military events, as interpreted by \citealt{Hibbs:2008aa}), and then update that belief based on new data, such as opinion polls. The specific technique of Bayesian inference is a way to mathematically solve the problem of weighting between different data based on how much information we think they contain, respectively.

The statistical tools and ideas used here have further applications in the political world. \citet{Lock:2010aa} note that ``an approach such as described here could be applied to study changes in public opinion and other phenomena with wide national swings and fairly stable spatial distributions relative to the national average,'' such as the application of these ideas by \citet{Lax:2009aa} to opinions and state policies on gay rights.

\begin{comment}

Other models incorporate more arcane techniques to, for example, examine voting turnout among demographic groups \citep{Ghitza:2013aa}, an important piece of information for political campaigns.

\textbf{How does it work?} Basic information ahead of time (what's the economy like? How many people are dying?) Incorporate opinion polls. Estimate state outcomes (the electoral college).

\end{comment}

\end{document}