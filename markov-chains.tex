\documentclass[thesis.tex]{subfiles}
\begin{document}

\section{Markov chains}

Let \(X_t\) be the value of a random variable at time \(t\). The random variable is a \term{Markov process} if the the distribution of \(X_{t+1}\) is dependent only on the current state, i.e., \[
	\Pr(X_{t+1} | X_t, \ldots, X_0)	= \Pr(X_{t+1} | X_t).
\] We call a sequence \((X_0, \ldots, X_n)\) of the variables generated by a Markov process, a \term{Markov chain}.

A chain is defined by its transition probabilities between states. We denote the probability of switching from state \(i\) to state \(j\) by \[
	\Pr(i \to j).
\]

Let \(\pi_j(t) = \Pr(X_t = j),\) and let \(\vec{\pi}(t)\) be all state space probabilites at time \(t\). Then by the \term{Chapman-Kolmogorov equation}, we see that \(
	\pi_i(t + 1) = \Pr(X_{t+1} = s_i)
	= \sum_k \Pr(X_{t+1} = s_i | X_t = s_k)\Pr(X_t = s_k)
	= \sum_k \Pr(k \to i)\pi_k(t).
\)

This becomes useful in matrix form. Let \(\mat{P}\) be the probability transition matrix whose \((i, j)\)-th element is \(\Pr(i \to j)\). Then \[
	\vec{\pi} (t + 1) = \pi(t) \mat{P},
\] and we see how to obtain subsequent steps of the Markov chain \[
	\vec{\pi}(t) = \vec{\pi}(t - n)\mat{P}^n = \vec{\pi}(0)\mat{P}^t.
\]

We call a Markov chain \term{irreducible} if all states communicate, i.e., we can go from any state to any other state.

We call a Markov chain \term{aperiodic} if the number of steps to move between any two states is not required to be a multiple of some integer other than one. In other words, there are no fixed-length cycles between states.

A finite Markov chain that is both irreducible and aperiodic has a stationary distribution.

\bigskip

\noindent\textbf{Example:} Suppose \(\Pr(rain | rain) = 0.5\), \(\Pr(sun | rain) = 0.25\), \(\Pr(cloud | rain) = 0.25\), and \[
	\mat{P} = \begin{pmatrix}
		0.5 & 0.25 & 0.25 \\
		0.5 & 0 & 0.5 \\
		0.25 & 0.25 & 0.5
	\end{pmatrix}.
\] Suppose today is sunny, or \(\vec{\pi}(0) = (0, 1, 0)\). Then \begin{equation}\begin{aligned}
	&\vec{\pi}(2) = \vec{\pi}(0) \mat{P}^2 = (0.375, 0.25, 0.375) \\
	&\vec{\pi}(7) = (0.4, 0.2, 0.4).
\end{aligned}\end{equation} Or, suppose today is rainy, \(\vec{\pi}(0) = (1, 0, 0)\). Then \begin{equation}\begin{aligned}
	&\vec{\pi}(2) = (0.4375, 0.1875, 0.375) \\
	&\vec{\pi}(7) = (0.4, 0.2, 0.4).
\end{aligned}\end{equation} We see a stationary distribution.

\bigskip

\begin{comment}Also, a Markov chain is irreducible, aperiodic, and positive recurrent has a stationary distribution (prove?).
\end{comment}

A sufficient condition to have a stationary distribution is the \term{detailed balance equation}, or \term{reversibility condition}, \[
	\Pr(j \to k)\pi_j^* = \Pr(k \to j)\pi_k^*,
\] where \(\vec{\pi}^*\) is the stationary distribution.

\begin{comment}
	\textbf{Claim:} \(\vec{\pi} = \vec{\pi}\mat{P}\).
\end{comment}

We extend this to a continuous state space by \[
	\int \Pr(x \to y) dy = 1,
\] and the Chapman-Kolmogrov equation becomes \[
	\pi_t(y) = \int \pi_{t-1}(x)\Pr(x \to y) dy,
\] or \[
	\vec{\pi^*}(y) = \int \vec{\pi^*} \Pr(x \to y) dy.
\]

\end{document}


\begin{comment}

\begin{definition}
  A Markov chain is irreducible if, for any two possible values, when started at one it is possible the chain will eventually take on the other. In other words, for any two possible values $i$ and $j$, there exists $m > 0$ such that \( \Pr(X_m = i | X_0 = j) > 0 \).
\end{definition}

\begin{definition}
  A Markov chain is periodic if it only takes on certain values with some period. In other words, for any two possible values $i$ and $j$, a timestep $m$, and a period $d$, \( \Pr(X_m = i | X_0 = j) = 0 \) unless $m$ is divisible by $d$. If $d = 1$, then we say the chain is aperiodic.
\end{definition}

%% TODO example about combinations of numbers from Gabe's notes?

We need one more trick that will allow us to understand the Metropolis-Hastings algorithm. Given an ergodic Markov chain with stationary distribution $\pi$, note that for all possible values $j$, $\pi_j = \sum_i \pi_i \Pr(i\to j)$. If this were not true, $\pi$ would not be the stationary distribution.

We also see that for all $j$, $\pi_j \ge 0$, and $\sum_i\Pr(j \to i) = 1$, otherwise $\pi$ would not be a distribution.

If there exists a set of probabilities $\pi$ for a Markov chain such that $\pi_i\Pr(i\to j) = \pi_j \Pr(j\to i)$, then by the above $\pi$ is the stationary distribution of the chain, and we call such a chain reversible. To see this, note that \( \sum_i \pi_i\Pr(i\to j) = \sum_i \pi_j\Pr(j\to i) = \pi_j \sum_i\Pr(j \to i) = \pi_j \).

\end{comment}