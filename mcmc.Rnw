<<echo=FALSE, cache=FALSE>>=
set_parent('thesis.Rnw')
@

\section{Markov-Chain Monte Carlo methods}

\subsection{Markov chains}

Let \(X_t\) be the value of a random variable at time \(t\). The random variable is a \term{Markov process} if the the distribution of \(X_{t+1}\) is dependent only on the current state, i.e., \[
  \Pr(X_{t+1} | X_t, \ldots, X_0)	= \Pr(X_{t+1} | X_t).
\] We call a sequence \((X_0, \ldots, X_n)\) of the variables generated by a Markov process, a \term{Markov chain}.

A chain is defined by its transition probabilities between states. We denote the probability of switching from state \(i\) to state \(j\) by \[
	\Pr(i \to j).
\]

Let \(\pi_j(t) = \Pr(X_t = j),\) and let \(\vec{\pi}(t)\) be all state space probabilites at time \(t\). Then by the \term{Chapman-Kolmogorov equation}, we see that \(
	\pi_i(t + 1) = \Pr(X_{t+1} = s_i)
	= \sum_k \Pr(X_{t+1} = s_i | X_t = s_k)\Pr(X_t = s_k)
	= \sum_k \Pr(k \to i)\pi_k(t).
\)

This becomes useful in matrix form. Let \(\mat{P}\) be the probability transition matrix whose \((i, j)\)-th element is \(\Pr(i \to j)\). Then \[
	\vec{\pi} (t + 1) = \pi(t) \mat{P},
\] and we see how to obtain subsequent steps of the Markov chain \[
	\vec{\pi}(t) = \vec{\pi}(t - n)\mat{P}^n = \vec{\pi}(0)\mat{P}^t.
\]

We call a Markov chain \term{irreducible} if all states communicate, i.e., we can go from any state to any other state.

We call a Markov chain \term{aperiodic} if the number of steps to move between any two states is not required to be a multiple of some integer other than one. In other words, there are no fixed-length cycles between states.

A finite Markov chain that is both irreducible and aperiodic has a stationary distribution.

\begin{example*}
Suppose \(\Pr(rain | rain) = 0.5\), \(\Pr(sun | rain) = 0.25\), \(\Pr(cloud | rain) = 0.25\), and \[
	\mat{P} = \begin{pmatrix}
		0.5 & 0.25 & 0.25 \\
		0.5 & 0 & 0.5 \\
		0.25 & 0.25 & 0.5
	\end{pmatrix}.
\] Suppose today is sunny, or \(\vec{\pi}(0) = (0, 1, 0)\). Then \begin{equation*}\begin{aligned}
	&\vec{\pi}(2) = \vec{\pi}(0) \mat{P}^2 = (0.375, 0.25, 0.375) \\
	&\vec{\pi}(7) = (0.4, 0.2, 0.4).
\end{aligned}\end{equation*} Or, suppose today is rainy, \(\vec{\pi}(0) = (1, 0, 0)\). Then \begin{equation*}\begin{aligned}
	&\vec{\pi}(2) = (0.4375, 0.1875, 0.375) \text{ and} \\
	&\vec{\pi}(7) = (0.4, 0.2, 0.4),
\end{aligned}\end{equation*} which is converging to a stationary distribution.
\end{example*}

\begin{comment}Also, a Markov chain is irreducible, aperiodic, and positive recurrent has a stationary distribution (prove?).
\end{comment}

A sufficient condition to have a stationary distribution is the \term{detailed balance equation}, or \term{reversibility condition}, \[
	\Pr(j \to k)\pi_j^* = \Pr(k \to j)\pi_k^*,
\] where \(\vec{\pi}^*\) is the stationary distribution.

\begin{comment}
	\textbf{Claim:} \(\vec{\pi} = \vec{\pi}\mat{P}\).
\end{comment}

We extend this to a continuous state space by \[
	\int \Pr(x \to y) dy = 1,
\] and the Chapman-Kolmogrov equation becomes \[
	\pi_t(y) = \int \pi_{t-1}(x)\Pr(x \to y) dy,
\] or \[
	\vec{\pi^*}(y) = \int \vec{\pi^*} \Pr(x \to y) dy.
\]

\begin{comment}

% \begin{definition}
  A Markov chain is irreducible if, for any two possible values, when started at one it is possible the chain will eventually take on the other. In other words, for any two possible values $i$ and $j$, there exists $m > 0$ such that \( \Pr(X_m = i | X_0 = j) > 0 \).
% \end{definition}

% \begin{definition}
  A Markov chain is periodic if it only takes on certain values with some period. In other words, for any two possible values $i$ and $j$, a timestep $m$, and a period $d$, \( \Pr(X_m = i | X_0 = j) = 0 \) unless $m$ is divisible by $d$. If $d = 1$, then we say the chain is aperiodic.
% \end{definition}

%% TODO example about combinations of numbers from Gabe's notes?

We need one more trick that will allow us to understand the Metropolis-Hastings algorithm. Given an ergodic Markov chain with stationary distribution $\pi$, note that for all possible values $j$, $\pi_j = \sum_i \pi_i \Pr(i\to j)$. If this were not true, $\pi$ would not be the stationary distribution.

We also see that for all $j$, $\pi_j \ge 0$, and $\sum_i\Pr(j \to i) = 1$, otherwise $\pi$ would not be a distribution.

If there exists a set of probabilities $\pi$ for a Markov chain such that $\pi_i\Pr(i\to j) = \pi_j \Pr(j\to i)$, then by the above $\pi$ is the stationary distribution of the chain, and we call such a chain reversible. To see this, note that \( \sum_i \pi_i\Pr(i\to j) = \sum_i \pi_j\Pr(j\to i) = \pi_j \sum_i\Pr(j \to i) = \pi_j \).

\end{comment}

\subsection{The Metropolis-Hastings algorithm}

Suppose we want to sample from the probability distribution \[
  p(\theta) = k\cdot f(\theta),
\] where K is some unknown normalizing constant.

We first present the \textbf{Metropolis} algorithm. Start with \[
  \theta^{(0)} : f(\theta^{(0)}) > 0.
\] Let \(q(\theta^{(1)} \to \theta^{(2)})\) be a \term{jumping distribution} (or \term{proposal}, or \term{candidate-generating} distibution) which is symmetric, \(q(\theta_1 \to \theta_2) = q(\theta_2 \to \theta_1)\). Then:

\begin{enumerate}
\item Sample a candidate point \(\theta^*\) from \(q\), which denotes the probability of returning \(\theta_2\) given a previous value of \(\theta_1\);

\item Calculate the ratio of density, \[
  \alpha = \frac{p(\theta^*)}{p(\theta^{(t-1)})} = \frac{f(\theta^*)}{f(\theta^{(t-1)})},
\] where the normalizing constant \(k\) cancels out;

\item If \(\theta^*\) increases the density (\(\alpha > 1\)), then accept the candidate and set \(\theta^{(t)} = \theta^*\). Otherwise, then accept the candidate with probability \(\alpha\), or pick a new candidate and try again.

\end{enumerate}

Another way to look at it is, accept a candidate with probability \[
  \alpha = \min\left[ \frac{f(\theta^*)}{f(\theta^{(t-1)})},\, 1 \right].
\]

The \term{Metropolis-Hastings} algorithm extends the above to work in the case where \(q(\theta_1 \to \theta_2)\) is any distribution, symmetric or not. Let the density ratio be \[
  \alpha = \min \left[ \frac{f(\theta^*)}{f(\theta^{(t-1)})}\frac{q(\theta^* \to \theta^{(t-1)})}{q(\theta^{(t-1)} \to \theta^*)},\, 1 \right].
\] In the case where \(q\) is symmetric, this becomes simply Metropolis.

\begin{theorem*}The Metropolis-Hastings algorithm generates a Markov chain with stationary distribution \(p\).
\end{theorem*}
\begin{proof}
 We show Metropolis-Hastings satisfies the detailed balance equation. Since we sample from \(q(\theta_1 \to \theta_2)\), and accept with probability \(\alpha(\theta_1, \theta_2)\), \[
  \Pr(\theta_1 \to \theta_2) = q(\theta_1 \to \theta_2)  \alpha(\theta_1, \theta_2) = q(\theta_1 \to \theta_2) \min \left[\frac{p(\theta_2)}{p(\theta_1)}\frac{q(\theta_2 \to \theta_1)}{q(\theta_1 \to \theta_2)} ,\, 1\right].
\] We want to show that \[
  \Pr(\theta_1 \to \theta_2) p(\theta_1) = \Pr(\theta_2 \to \theta_1)p(\theta_2),
\] or \[
  q(\theta_1 \to \theta_2)\alpha(\theta_1, \theta_2) p(\theta_1) = q(\theta_2 \to \theta_1) \alpha(\theta_2, \theta_1) p(\theta_2).
\] Assume without loss of generality that \(\alpha(\theta_1, \theta_2) < 1\). Then terms cancel and we see the equality.
\end{proof}

\begin{example*} Consider the distribution, \[
  p(\theta) = C \cdot \theta^{-n/2} \cdot \exp \left(-\frac{a}{2\theta}\right),
\] with \(n = 5\) and \(a = 4\).

Take a Metropolis candidate distribution, the uniform distribution from 0 to 100. We know \(p\) has tails outside this range, but assume that they're negligible.

Now, run the Metropolis algorithm: Let \(\theta^{(0)} = 1\), and suppose \(\theta^* = 39.82\). Then \[
  \alpha = \min\left[ \frac{p(\theta^*)}{p(\theta^{(t-1)})} ,\, 1\right] = \min\left[ \frac{(\theta^*)^{-5/2} \exp\left( -\frac{2}{\theta^*} \right)}{(\theta^{(t-1)})^{-5/2} \exp\left( -\frac{2}{\theta^{(t-1)}} \right)},\, 1 \right] = 0.0007,
\] so we accept \(\theta^*\) with probability 0.0007.

The first 500 steps of this algorithm are shown in the left side of Figure \ref{fig:mixing}. Note the long flat periods, where all candidate values are rejected. This chain is called \textbf{poorly-mixing}.

Suppose that we instead use \(\chi^2_1\) as the candidate distribution. This distribution is no longer symmetric, so we must use Metropolis-Hastings instead of just Metropolis. A resulting sample run is shown in the right side of  Figure \ref{fig:mixing}. The series of samples looks like \textbf{white noise}, and we call this chain \textbf{well-mixing}.

\end{example*}

<<mixing, fig.cap='Poorly-mixing and well-mixing Metropolis chains', out.width='.49\\linewidth', echo=FALSE, cache=TRUE, fig.show='hold'>>=
# Poorly-mixing
set.seed(47)

p <- function(theta) theta^(-2.5) * exp(-2 / theta)

next_theta <- function(theta0) {
  theta <- runif(1, min=0, max=100)
  print
  alpha <- min(p(theta) / p(theta0), 1)
  x <- runif(1)
  if (x <= alpha) {
    return(theta)
  } else {
    return(theta0)
  }
}

thetas <- rep(NA, 500)
thetas[1] <- 1
for (i in 2:500) {
  thetas[i] <- next_theta(thetas[i-1])
}

plot(thetas, type='l', lwd=2, bty='n', xlab='n', ylab='Theta', cex.lab=1.5, cex.axis=1.5)

# Well-mixing
set.seed(238)
library(functional)

p <- function(theta) theta^(-2.5) * exp(-2 / theta)
q <- Curry(pchisq, df=1)

next_theta <- function(theta0) {
  theta <- rchisq(1, df=1)
  alpha <- min(p(theta) * q(theta0) / p(theta0) / q(theta), 1)
  x <- runif(1)
  if (x <= alpha) {
    return(theta)
  } else {
    return(theta0)
  }
}

thetas <- rep(NA, 500)
thetas[1] <- 1
for (i in 2:500) {
  thetas[i] <- next_theta(thetas[i-1])
}

plot(thetas, type='l', lwd=2, bty='n', xlab='n', ylab='Theta', cex.lab=1.5, cex.axis=1.5)

# p <- function(x) Curry(dnorm, mean=2, sd=1)(x) + Curry(dnorm, mean=-3, sd=2)(x)
# Check it out a bimodal distribution
# Could be cool for simulated annealing or random walk metropolis-hastings or
# multiple chains or whatever
@

\begin{comment}
Plot of first 500 values shows long flat periods---this is called \term{poorly-mixing}. TODO

In contrast, suppose we use \(\chi^2(1)\) as the proposal distribution. This no longer meets the assumptions of Metroplis (\(q(\theta_2) \ne q(\theta_1)\)), so we use Metropolis-Hastings. Now the plot looks morel ike white noise, and we say that it is \term{well-mixing}. TODO
\end{comment}

\begin{comment}

The Metropolis-Hastings algorithm is a technique in the family of Markov-Chain Monte Carlo (MCMC) methods that allows numerically approximating a probability distribution, often used when the distribution is difficult to sample from directly.

The algorithm generates sequences of samples in such a way that their distribution converges to the actual distribution being analyzed. Each sample is dependent only on the sample before it, making the sequence a Markov chain.

The Metropolis-Hastings algorithm works as follows, given a stationary distribution with PDF $f$ (the distribution to sample from) and a proposal distribution $g$:

\begin{enumerate}
  \item Given $X_t = x$ at time $t$
  \item Sample $y$ from the proposal distribution with $g(y|X_t = x)$
  \item Calculate the Metropolis-Hastings ratio: \[
    R(x, y) = \min\left( \dfrac{f(y)g(x|y)}{f(x)g(y|x)}, 1 \right)
  \]
  and with probability $R(x, y)$, let $X_{t+1} = y$. Otherwise let $X_{t+1} = X_t$.
\end{enumerate}

This algorithm produces a sequence of samples whose distribution converges to $f$.

Let $x_1$ and $x_2$ be two possible values of the proposal distribution. Suppose $f(x_1)g(x_2|x_1) < f(x_2)g(x_1|x_2)$. Then the transition from $x_1$ to $x_2$ has joint distribution the change of starting with $x_1$ and being offered $x_2$, multiplied by the M-H ratio: or, in other words $f(x_1)g(x_2|x_1)$. The transition the other way has a joint distribution of \[ f(x_2)g(x_2|x_1)\dfrac{f(x_1)g(x_2|x_1)}{f(x_2)g(x_1|x_2)} = f(x_1)g(x_2|x_1)\]
and thus by the statements on Markov chains above this chain is reversible and $f$ is its stationary distribution.

%% TODO: Wow that's weird to understand. Specifically the bit about reversibility and how to prove that.

%% TODO: Need more about how to pick a proposal distribution.

\end{comment}

\subsection{Gibbs sampling}

Consider a bivariate random variable \((x, y)\), for which we want to find the joint distribution \begin{equation*}\begin{aligned}
  p(x, y).
\end{aligned}\end{equation*} It's easier to calculate the conditional distributions \(p(x | y)\) and \(p(y | x)\) than to integrate to get the marginal, \(p(x) = \int p(x, y) dy\).

The Gibbs sampling algorithm for two variables is as follows:

\begin{enumerate}
	\item Start with some value for \(y\), \(y^{(0)}\);
	\item Sample \(x^{(0)} \sim p(x | y = y^{(0)})\);
	\item Sample \(y^{(1)} \sim p(y | x = x^{(0)}\);
  \item Sample \(x^{(1)} \sim p(x | y = y^{(1)})\);
	\item And so on.
\end{enumerate}

The general multivariate version is analagous.

In the next section we provide a concrete example of finding the conditional distributions for the Gibbs sampler. In general, we find the full joint distribution, which by definition is proportional to the conditional distribution of each variable. Then for each conditional distribution, we simply drop unneeded terms into the normalization constant and hope to get a conjugate posterior.

\begin{theorem*} The Gibbs sampler is a Markov process with stationary distribution the true joint distribution.\end{theorem*}
\begin{proof}We show the detailed balance equations hold for sampling \(x\) with regards to the joint distribution \(p(x, y)\), \begin{equation*}\begin{aligned}
	p(x_2, y) p(x_1| y) = p(x_1, y) p(x_2 | y).
\end{aligned}\end{equation*} By the definition of conditional probability, this is \begin{equation*}\begin{aligned}
	p(x_2 | y) p(y) \cdot p(x_1| y) = p(x_1 | y) p( y) \cdot p(x_2 | y),
\end{aligned}\end{equation*} so we're done.\end{proof}

\subsection{Fitting a random walk preference model with house biases}

\begin{comment}
Talking about the models in \cite{Strauss:2007aa} and \cite{Jackman:2005aa}.
\end{comment}

We model popular opinion at some time until Election Day \(t\) (where we denote Election Day by \(t = 0\)). We assume that each time period, popular opnion undergoes some random ``shock'' with variance \(\omega^2\). In other words, \[
  \alpha_t \sim \Normal(\alpha_{t-1},\, \omega^2).
\]

We then model opinion polls \(y_i\) as being sampled from popular opinion at that time, with some house bias \(\delta_j\), or \[
	y_i \sim \Normal(\alpha_t + \delta_j,\, s_i^2),
\] where \(s_i^2\) is the poll variance \(\frac{y_i(1-y_i)}{q_i}\) and \(q_i\) is the number of respondents in the poll. We assume that house biases cancel out, i.e., \(\sum \delta_j = 0\).

\citet{Strauss:2007aa} models the national preference for a Republican president as a reverse random walk, taking $\alpha_0$ as the election result. He builds off a model initially proposed by \citet{Jackman:2005aa}, who uses a forward random walk to predict Australian parliamentary elections. The difference in nomenclature derives from where each assigns priors: \citet{Jackman:2005aa} assigns a prior at the earliest time step of his model, whereas \citet{Strauss:2007aa} assigns a prior on the final value of his model.

\citet{Strauss:2007aa} estimates priors on the shock as \(
\omega \sim \Uniform(0, (0.01)^2)\) and on house bias as  \(
\delta_j \sim \N(0, (0.1)^2)\).

\begin{comment}
\emph{TODO talk about priors at all? Or just reference from before?} He uses the Campbell (\citeyear{Campbell:1992aa,Campbell:2006aa}) prior on the national outcome, predicted off of a number of indicators. This gives \(\alpha_0 \sim \N(0.522, (0.0253)^2)\) for the 2008 election (calculated before polling information is available).
\end{comment}

We obtain posterior distributions from this model using a Gibbs sampler. We first obtain the full joint posterior distribution for all parameters, given our polling data. We then condition for each parameter, and find that our conditional posteriors are all simple conjugate distributions.

Given our model above, we see our joint distribution \[
	\Pr(\alpha_0, \ldots, \alpha_n, \delta_i, \ldots, \delta_J, \omega | y_1, \ldots, y_P)) = \Pr(\Theta | D),
\] where \(n\) is our total number of time periods, \(J\) is the number of organizations, and \(P\) is the number of polls we have data for. For convenience, we let \(D\) denote all data, and \(\Theta\) denote all parameters. By Bayes' Theorem, this is proportional to \[
	\Pr(D | \Theta ) \Pr(\Theta).
\]

We assume that house biases are independent from popular opinion at each time period, and from the variance of shock to opinion (which may not be true, but is too complicated otherwise). By the definition of conditional probability, \[
	\Pr(A, B) = \Pr(A | B)\Pr(B),
\] and our joint distribution becomes \[
	\propto \Pr(D | \Theta) \left[\prod \Pr(\delta_j) \right] \Pr(\alpha_0, \ldots, \alpha_n | \omega) \Pr(\omega).
\] Again, by the definition of conditional probability, the distribution becomes \[
	\propto \left[\prod_{i=1}^P \Pr(y_i | \Theta)\right] \left[\prod_{j=1}^J \Pr(\delta_j) \right]\left[\prod_{t=1}^n \Pr(\alpha_t | \alpha_{t-1}, \omega)\right] \Pr(\alpha_0) \Pr(\omega).
\]

To find the conditional probabilites, we take the conditional as proportional to the joint distribution, and drop terms that don't matter.

For \(\alpha_0\), we see that \[
	\Pr(\alpha_0 | \Theta_{-\alpha_0}, D) \propto \Pr(\alpha_0)\Pr(\alpha_1 | \alpha_0, \omega)\prod \Pr(y_i | \alpha_0, \delta_j),
\] for all polls \(y_i\) that were conducted at time period \(\alpha_0\). Note that this is simply a conjugate posterior, so we see that \(\alpha_0 | \Theta_{-\alpha_0}, D\) has a Normal distribution with variance \[
\left(\frac{1}{\sigma^2_0} + \frac{1}{\omega^2} + \sum\frac{1}{s_i^2}\right)^{-1}
\] and mean \[
	\left(\frac{\mu_0}{\sigma^2_0} + \frac{\alpha_1}{\omega^2} + \sum \frac{y_i - \delta_j}{s_i^2}\right)\Var(\alpha_0 | \Theta_{-\alpha_0}, D).
\] We see conditional distributions for the rest of the \(\alpha_t\) similarly.

For \(\omega\), we see that \[
	\Pr(\omega^2 | \Theta_{-\omega}, D) \propto \Pr(\omega) \prod_{t=1}^n \Pr(\alpha_t | \alpha_{t-1}, \omega),
\] or, letting \(\mathbb{I}_\omega = \Pr(\omega\)), \begin{align}
	&\propto \mathbb{I}_\omega \cdot \prod_{t=1}^n (\omega^2)^{-1/2} \exp\left[ - (\omega^2)^{-1} \frac{(\alpha_t - \alpha_{t-1})^2}{2} \right] \\
	&= \mathbb{I}_\omega \cdot (\omega^2)^{-n/2} \exp\left[ - (\omega^2)^{-1} \frac{\sum_{t=1}^n(\alpha_t - \alpha_{t-1})^2}{2} \right],
\end{align} which is the probability density function of an Inverse Gamma distribution, restricted to some range. Thus we say \[
	\omega^2 \sim \mathbb{I}_\omega \cdot \InvGamma\left( \frac{n - 2}{2},\, \frac{\sum_{t=1}^n(\alpha_t - \alpha_{t-1})^2}{2} \right).
\] When sampling this, we can the assumption that the Inverse Gamma will not have much mass outside of the prior on \(\omega\), and can be approximated by dropping all samples outside of \(\mathbb{I}_\omega\) and resampling, or we can use another Markov Chain Monte Carlo method such as Metropolis-Hastings or rejection sampling.

Finally, for \(\delta_j\), we see that \begin{align}
	\Pr(\delta_j | \Theta_{-\delta_j}, D) \propto \Pr(\delta_j) \prod \Pr(y_i | \alpha_0, \delta_j)
\end{align} for all polls \(y_i\) from the organization indexed by \(j\). Given a prior \(\delta_j \sim \Normal(0, d^2)\), we see we have a conjugate posterior, so  \(\delta_j | \Theta_{-\delta_j} \) has a Normal distribution with variance \begin{align}
	\left(\frac{1}{d^2} + \sum\frac{1}{s_i^2}\right)^{-1}
\end{align} and mean \begin{align}
	\left(\sum\frac{y_i - \alpha_t}{s_i^2}\right) \Var(\delta_j | \Theta_{-\delta_j}).
\end{align}

<<gibbs, echo=FALSE, cache=TRUE, fig.cap="Gibbs sampler run on simulated election model. The bold line in the middle is true voter preference, the symbols above and below it are polls conducted by different organizations, and the thin lines are the point estimate of voter preference and its 90\\% CIs.", fig.pos="h">>=
data <- dget('strausswalk.data')
plot(data$true, type="l", xlim=c(100, 1), xlab="Time", ylab="Voter preference", ylim=c(0.4, 0.6), lwd=3, bty='n')
points(data$giddyup, pch=3)
points(data$rasputin, pch=1)
pref <- data$gibbs
lines(seq(1,100), colMeans(pref))
lines(seq(1,100), apply(pref, 2, mean) + (apply(pref, 2, sd) * qnorm(0.90)))
lines(seq(1,100), apply(pref, 2, mean) - (apply(pref, 2, sd) * qnorm(0.90)))
@

We graph the results of this model on simulated election data in Figure \ref{fig:gibbs}.

\begin{comment}
	The conditional distributions for the Gibbs sampler are:
\begin{enumerate}
  \item $\alpha_t$: The conditional probabilities for each time step $\alpha_t$ are completely determined by the values on either side, $\alpha_{t-1}$ and $\alpha_{t+1}$ (except for the first and last, which are determined by only the next or previous time step, respectively), and any data points at that time step.

Then the value at each time step $\alpha_t$ is the conjugate posterior for the mean of normal variables with known variance (\hyperref[sec:conj:gaussianwithprecision]{Appendix}), combining the time steps on either side, any sample points, and a prior if we have assigned one to that time step.

Thus each $\alpha_t$ is a mix of the any time periods adjoining, $\N(\alpha_{t+1}, \omega^2)$ and $\N(\alpha_{t-1}, \omega^2)$ and of any polls for that time period, debiased, $\N(y_i - \delta_j, s_i^2)$.
    
  \item $\tau$, where $\tau = \omega^{-2}$: The shock variance $\omega^2$ is given by the conjugate posterior for the variance of normal variables with known mean (\hyperref[sec:conj:gaussianwithmean]{Appendix}), where the values we are looking at are $(\alpha_t - \alpha_{t-1})^2$.

  In this case we have the variance between neighboring time periods, $\frac{\sum(\alpha_{t+1} - \alpha_t)^2}{T-2}$, and the prior means we resample until we get $\omega^2 < (0.01)^2$.
  
  \item $\delta_j$: The house biases are a combination of the prior $\N(0, (0.1)^2)$ and the differences of all polls by that organization from true preferences, $\N(y_i - \alpha_t, s_i^2)$.
\end{enumerate}
\end{comment}