\documentclass[thesis.tex]{subfiles}
\begin{document}

\section{Gibbs sampling for estimating house biases}

Talking about the models in \cite{Strauss:2007aa} and \cite{Jackman:2005aa}.

We model popular opinion at some time until Election Day \(t\) (where we denote Election Day by \(t = 0\)). We assume that each time period, popular opnion undergoes some random ``shock'' with variance \(\omega^2\). In other words, \[
	\alpha_t \sim \Normal(\alpha_{t-1},\, \omega^2).
\]

We then model opinion polls \(y_i\) as being sampled from popular opinion at that time, with some house bias \(\delta_j\), or \[
	y_i \sim \Normal(\alpha_t + \delta_j,\, s_i^2),
\] where \(s_i^2\) is the poll variance \(\frac{y_i(1-y_i)}{q_i}\) and \(q_i\) is the number of respondents in the poll. We assume that house biases cancel out, i.e., \(\sum \delta_j = 0\).

\citet{Strauss:2007aa} models the national preference for a Republican president as a reverse random walk, taking $\alpha_0$ as the election result. He builds off a model initially proposed by \citet{Jackman:2005aa}, who uses a forward random walk to predict Australian parliamentary elections. The difference in nomenclature derives from where each assigns priors: \citet{Jackman:2005aa} assigns a prior at the earliest time step of his model, whereas \citet{Strauss:2007aa} assigns a prior on the final value of his model.

\citet{Strauss:2007aa} estimates priors on the shock as \(
\omega \sim \Uniform(0, (0.01)^2)\) and on house bias as  \(
\delta_j \sim \N(0, (0.1)^2)\). He justifies this by noting that
\begin{quote}
The day-to-day shocks of the campaign are assumed to fall within plus or minus 2 percentage points 95\% of the time (at a maximum). While specific events (e.g., conventions, debates) might cross this threshold, those are the exceptions, not the norms. [Footnote: Even conventions, which last four days, might not produce effects larger than two percentage points per day.] The prior on house effects assumes that 95\% of bias is less than 20\% (in either direction): all reputable polling organizations should easily pass that standard.
\end{quote}

\begin{comment}
\emph{TODO talk about priors at all? Or just reference from before?} He uses the Campbell (\citeyear{Campbell:1992aa,Campbell:2006aa}) prior on the national outcome, predicted off of a number of indicators. This gives \(\alpha_0 \sim \N(0.522, (0.0253)^2)\) for the 2008 election (calculated before polling information is available).
\end{comment}

We obtain posterior distributions from this model using a Gibbs sampler. We first obtain the full joint posterior distribution for all parameters, given our polling data. We then condition for each parameter, and find that our conditional posteriors are all simple conjugate distributions.

Given our model above, we see our joint distribution \[
	\Pr(\alpha_0, \ldots, \alpha_n, \delta_i, \ldots, \delta_J, \omega | y_1, \ldots, y_P)) = \Pr(\Theta | D),
\] where \(n\) is our total number of time periods, \(J\) is the number of organizations, and \(P\) is the number of polls we have data for. For convenience, we let \(D\) denote all data, and \(\Theta\) denote all parameters. By Bayes' Theorem, this is proportional to \[
	\propto \Pr(D | \Theta ) \Pr(\Theta).
\]

\bigskip

\noindent\textbf{Definition (Conditional probability):} \(\Pr(A, B) = \Pr(A | B)\Pr(B)\).

\bigskip

We assume that house biases are independent from popular opinion at each time period, and from the variance of shock to opinion (which may not be true, but is too complicated otherwise). By the definition of conditional probability, \[
	\Pr(A, B) = \Pr(A | B)\Pr(B),
\] and our joint distribution becomes \[
	\propto \Pr(D | \Theta) \left[\prod \Pr(\delta_j) \right] \Pr(\alpha_0, \ldots, \alpha_n | \omega) \Pr(\omega).
\] Again, by the definition of conditional probability, the distribution becomes \[
	\propto \left[\prod_{i=1}^P \Pr(y_i | \Theta)\right] \left[\prod_{j=1}^J \Pr(\delta_j) \right]\left[\prod_{t=1}^n \Pr(\alpha_t | \alpha_{t-1}, \omega)\right] \Pr(\alpha_0) \Pr(\omega).
\]

To find the conditional probabilites, we take the conditional as proportional to the joint distribution, and drop terms that don't matter.

For \(\alpha_0\), we see that \[
	\Pr(\alpha_0 | \Theta_{-\alpha_0}, D) \propto \Pr(\alpha_0)\Pr(\alpha_1 | \alpha_0, \omega)\prod \Pr(y_i | \alpha_0, \delta_j),
\] for all polls \(y_i\) that were conducted at time period \(\alpha_0\). Note that this is simply a conjugate posterior, so we see that \(\alpha_0 | \Theta_{-\alpha_0}, D\) has a Normal distribution with variance \[
\left(\frac{1}{\sigma^2_0} + \frac{1}{\omega^2} + \sum\frac{1}{s_i^2}\right)^{-1}
\] and mean \[
	\left(\frac{\mu_0}{\sigma^2_0} + \frac{\alpha_1}{\omega^2} + \sum \frac{y_i - \delta_j}{s_i^2}\right)\Var(\alpha_0 | \Theta_{-\alpha_0}, D).
\] We see conditional distributions for the rest of the \(\alpha_t\) similarly.

For \(\omega\), we see that \[
	\Pr(\omega^2 | \Theta_{-\omega}, D) \propto \Pr(\omega) \prod_{t=1}^n \Pr(\alpha_t | \alpha_{t-1}, \omega),
\] or, letting \(\mathbb{I}_\omega = \Pr(\omega\)), \begin{align}
	&\propto \mathbb{I}_\omega \cdot \prod_{t=1}^n (\omega^2)^{-1/2} \exp\left[ - (\omega^2)^{-1} \frac{(\alpha_t - \alpha_{t-1})^2}{2} \right] \\
	&= \mathbb{I}_\omega \cdot (\omega^2)^{-n/2} \exp\left[ - (\omega^2)^{-1} \frac{\sum_{t=1}^n(\alpha_t - \alpha_{t-1})^2}{2} \right],
\end{align} which is the probability density function of an Inverse Gamma distribution, restricted to some range. Thus we say \[
	\omega^2 \sim \mathbb{I}_\omega \cdot \InvGamma\left( \frac{n - 2}{2},\, \frac{\sum_{t=1}^n(\alpha_t - \alpha_{t-1})^2}{2} \right).
\] When sampling this, we can the assumption that the Inverse Gamma will not have much mass outside of the prior on \(\omega\), and can be approximated by dropping all samples outside of \(\mathbb{I}_\omega\) and resampling, or we can use another Markov Chain Monte Carlo method such as Metropolis-Hastings or rejection sampling.

Finally, for \(\delta_j\), we see that \begin{align}
	\Pr(\delta_j | \Theta_{-\delta_j}, D) \propto \Pr(\delta_j) \prod \Pr(y_i | \alpha_0, \delta_j)
\end{align} for all polls \(y_i\) from the organization indexed by \(j\). Given a prior \(\delta_j \sim \Normal(0, d^2)\), we see we have a conjugate posterior, so  \(\delta_j | \Theta_{-\delta_j} \) has a Normal distribution with variance \begin{align}
	\left(\frac{1}{d^2} + \sum\frac{1}{s_i^2}\right)^{-1}
\end{align} and mean \begin{align}
	\left(\sum\frac{y_i - \alpha_t}{s_i^2}\right) \Var(\delta_j | \Theta_{-\delta_j})
\end{align}


\begin{comment}
	The conditional distributions for the Gibbs sampler are:
\begin{enumerate}
  \item $\alpha_t$: The conditional probabilities for each time step $\alpha_t$ are completely determined by the values on either side, $\alpha_{t-1}$ and $\alpha_{t+1}$ (except for the first and last, which are determined by only the next or previous time step, respectively), and any data points at that time step.

Then the value at each time step $\alpha_t$ is the conjugate posterior for the mean of normal variables with known variance (\hyperref[sec:conj:gaussianwithprecision]{Appendix}), combining the time steps on either side, any sample points, and a prior if we have assigned one to that time step.

Thus each $\alpha_t$ is a mix of the any time periods adjoining, $\N(\alpha_{t+1}, \omega^2)$ and $\N(\alpha_{t-1}, \omega^2)$ and of any polls for that time period, debiased, $\N(y_i - \delta_j, s_i^2)$.
    
  \item $\tau$, where $\tau = \omega^{-2}$: The shock variance $\omega^2$ is given by the conjugate posterior for the variance of normal variables with known mean (\hyperref[sec:conj:gaussianwithmean]{Appendix}), where the values we are looking at are $(\alpha_t - \alpha_{t-1})^2$.

  In this case we have the variance between neighboring time periods, $\frac{\sum(\alpha_{t+1} - \alpha_t)^2}{T-2}$, and the prior means we resample until we get $\omega^2 < (0.01)^2$.
  
  \item $\delta_j$: The house biases are a combination of the prior $\N(0, (0.1)^2)$ and the differences of all polls by that organization from true preferences, $\N(y_i - \alpha_t, s_i^2)$.
\end{enumerate}
\end{comment}

\end{document}